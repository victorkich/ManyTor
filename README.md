# The Algorithms
In this project i chose to train and test 2 great algorithms to solve the continuous inverse kinematics with multiple objectives.
These algorithms is Proximal Policy Optimization (PPO) and Deep Deterministic Policy Gradient (DDPG).

# The Environment
For general purpose, i decided to create a new environmnet instead use one environment already created.
Is this project i use matplotlib to plot and simulate links, joints and objectives, as well as ground and projection (terminal shadow!?).
My environment consist in one manipulator with 4 joints (3+terminal) with the following Denavit-Hartember parameters:

<p align="center"> 
<img src="https://i.imgur.com/IyulesQ.png"/>
</p>

# The Objectives
The environment objectives consist in 10 random points refreshing when all points is colected.
Each point have one fixed reward and one variable reward, but that will be explained soon.

<p align="center"> 
<img src="https://media.giphy.com/media/Wonv0YvrM5Djy6XkXW/giphy.webp"/>
</p>

# The Reward
...

# PPO
...
...
...
<p align="center"> 
<img src=""/>
</p>

# DDPG
...
...
...
<p align="center"> 
<img src=""/>
</p>
